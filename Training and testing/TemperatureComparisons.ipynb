{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def training_proc(num_training_updates,beta_i,lambd,model):\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    train_res_recon_error = []\n",
    "    train_res_perplexity = []\n",
    "    \n",
    "    for i in (xrange(num_training_updates)):\n",
    "        (data, _) = next(iter(training_loader))\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        beta = beta_i \n",
    "        #beta = beta_i* ((i+1)/num_training_updates) \n",
    "        \n",
    "        vq_loss, data_recon, perplexity,_,_ = model(data,beta,lambd)\n",
    "        recon_error = F.mse_loss(data_recon, data) / data_variance\n",
    "        loss = recon_error + vq_loss\n",
    "        loss.backward()\n",
    "    \n",
    "        optimizer.step()\n",
    "    \n",
    "        train_res_recon_error.append(recon_error.item())\n",
    "        train_res_perplexity.append(perplexity.item())\n",
    "    \n",
    "        if (i+1) % 100 == 0:\n",
    "            print('%d iterations' % (i+1))\n",
    "            print('recon_error: %.3f' % np.mean(train_res_recon_error[-100:]))\n",
    "            print('perplexity: %.3f' % np.mean(train_res_perplexity[-100:]))\n",
    "            print()\n",
    "            \n",
    "    return(train_res_recon_error,train_res_perplexity)\n",
    "\n",
    "def test_proc(num_test_updates,beta,lambd,model): \n",
    "    test_res_recon_error = []\n",
    "    test_res_perplexity = []\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "\n",
    "        for i in (xrange(num_test_updates)):\n",
    "            (data, _) = next(iter(validation_loader))\n",
    "            data = data.to(device)\n",
    "            optimizer.zero_grad()\n",
    "                \n",
    "            vq_loss, data_recon, perplexity,_,_ = model(data,beta,lambd)\n",
    "            recon_error = F.mse_loss(data_recon, data) / data_variance\n",
    "            loss = recon_error + vq_loss    \n",
    "            \n",
    "            test_res_recon_error.append(recon_error.item())\n",
    "            test_res_perplexity.append(perplexity.item())\n",
    "        \n",
    "\n",
    "    return(np.mean(test_res_recon_error),np.mean(perplexity.item()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_training_updates = 4000\n",
    "\n",
    "\n",
    "beta_i = 2\n",
    "lambd = 0.001\n",
    "\n",
    "recons_t = []\n",
    "perps_t = []\n",
    "models = []\n",
    "\n",
    "#### test params:\n",
    "\n",
    "beta_v = beta_i\n",
    "num_test_updates = 50\n",
    "\n",
    "recons_v = []\n",
    "perps_v = []\n",
    "betas = []\n",
    "\n",
    "\n",
    "\n",
    "for k in tqdm(range(10)):\n",
    "    if k == 0: \n",
    "        alpha = 0\n",
    "    else: \n",
    "        alpha = 1\n",
    "        \n",
    "    beta_g = (alpha*beta_i**(k-1))\n",
    "    \n",
    "    model = Model(num_hiddens, num_residual_layers, num_residual_hiddens,\n",
    "              num_embeddings, embedding_dim,\n",
    "              commitment_cost, decay).to(device)\n",
    "    \n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate, amsgrad=False)\n",
    "    \n",
    "    recon_t,perp_t = training_proc(num_training_updates,beta_g,lambd,model)\n",
    "    recons_t.append(recon_t)\n",
    "    perps_t.append(perp_t)\n",
    "\n",
    "    recon_v,perp_v = test_proc(num_test_updates,beta_g,lambd,model)\n",
    "    recons_v.append(recon_v)\n",
    "    print(recon_v)\n",
    "    perps_v.append(perp_v)\n",
    "    betas.append(alpha*beta_i**(k-1))\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmap = plt.cm.coolwarm  # You can also use plt.cm.bwr for a similar effect\n",
    "colors = cmap(1-np.linspace(0, 1, len(recons_t)))\n",
    "\n",
    "f = plt.figure(figsize=(16,8))\n",
    "\n",
    "# First subplot: Smoothed NMSE for all recon_v lists\n",
    "ax1 = f.add_subplot(1, 2, 1)\n",
    "for k,recon in enumerate(recons_t):\n",
    "    train_res_recon_error_smooth = uniform_filter1d(recon, size=10, mode='nearest')\n",
    "    beta_val = (betas[k]) #(torch.round(k*beta_i,decimals = 2).item())\n",
    "    ax1.plot(train_res_recon_error_smooth, label = r'$\\beta=$'+str(round(beta_val)),color=colors[k]) #'β ={:.2f}'.format(beta_val))\n",
    "\n",
    "ax1.legend()\n",
    "ax1.set_yscale('log') \n",
    "ax1.set_title('Smoothed NMSE.') \n",
    "ax1.set_xlabel('iteration') \n",
    "\n",
    "# Second subplot: Smoothed Average codebook usage (perplexity) for all perp_v lists\n",
    "ax2 = f.add_subplot(1, 2, 2)\n",
    "for k,perp in enumerate(perps_t):\n",
    "    train_res_perplexity_smooth = uniform_filter1d(perp, size=10, mode='nearest')\n",
    "    beta_val = betas[k]#(torch.round(k*beta_i,decimals = 2).item())\n",
    "    ax2.plot(train_res_perplexity_smooth, label = r'$\\beta=$'+str(beta_val),color=colors[k]) #'β={:.2f}'.format(beta_val))\n",
    "\n",
    "ax2.legend()\n",
    "ax2.set_title('Smoothed Average codebook usage (perplexity).')\n",
    "ax2.set_xlabel('iteration')\n",
    "#ax2.plot(betas_smooth, color='red', label='Beta')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "betas = [k for k in range(10)]\n",
    "\n",
    "print(betas)\n",
    "f = plt.figure(figsize=(14,5))\n",
    "ax = f.add_subplot(1,2,1)\n",
    "ax.scatter(betas,recons_v)\n",
    "ax.plot(betas,recons_v,alpha = 0.3)\n",
    "ax.set_yscale('log')\n",
    "ax.set_title('Smoothed NMSE (test)')\n",
    "ax.set_xlabel(r'$\\log_2(\\beta)$')\n",
    "\n",
    "ax = f.add_subplot(1,2,2)\n",
    "ax.scatter(betas,perps_v)\n",
    "ax.plot(betas,perps_v,alpha = 0.3)\n",
    "ax.set_title('Smoothed Average codebook usage (perplexity) (test)')\n",
    "ax.set_xlabel(r'$\\log_2(\\beta)$')\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
