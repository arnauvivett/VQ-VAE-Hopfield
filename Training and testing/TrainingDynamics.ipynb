{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_res_recon_error = []\n",
    "train_res_perplexity = []\n",
    "betas = []\n",
    "ent = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(num_hiddens, num_residual_layers, num_residual_hiddens,\n",
    "              num_embeddings, embedding_dim,\n",
    "              commitment_cost, decay).to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate, amsgrad=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_training_updates = 4000\n",
    "\n",
    "model.train()\n",
    "\n",
    "for i in tqdm(xrange(num_training_updates)):\n",
    "    (data, _) = next(iter(training_loader))\n",
    "    data = data.to(device)\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    \n",
    "    #beta = 32\n",
    "    lambd = 0.001#*beta   \n",
    "\n",
    "     #/  \n",
    "    beta = 32* ((i+1)/num_training_updates)\n",
    "    \n",
    "\n",
    "    \"\"\"   #\\/\n",
    "    if i < num_training_updates/2:\n",
    "        beta = 9.1-2*9*(i+1)/num_training_updates \n",
    "    elif i > num_training_updates/2:\n",
    "        beta = 0.1+2*9*(i-num_training_updates/2)/num_training_updates \n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"  #/\\\n",
    "    if i < num_training_updates/2:\n",
    "        beta = 1+16*(i+1)/num_training_updates \n",
    "    elif i > num_training_updates/2:\n",
    "        beta = 9-16*(i-num_training_updates/2)/num_training_updates \n",
    "    \"\"\"\n",
    "    \"\"\"  #_/\n",
    "    if i < num_training_updates/2:\n",
    "        beta = 2.5\n",
    "    elif i > num_training_updates/2:\n",
    "        beta = 2.5+16*(i-num_training_updates/2)/num_training_updates \n",
    "    \"\"\"\n",
    "        \n",
    "    vq_loss, data_recon, perplexity,entropy,encodings = model(data,beta,lambd)\n",
    "    recon_error = F.mse_loss(data_recon, data) / data_variance\n",
    "    loss = recon_error + vq_loss\n",
    "    loss.backward()\n",
    "\n",
    "    optimizer.step()\n",
    "\n",
    "    train_res_recon_error.append(recon_error.item())\n",
    "    train_res_perplexity.append(perplexity.item())\n",
    "    ent.append(entropy.item())\n",
    "    betas.append(beta)\n",
    "    \n",
    "    if (i+1) % 100 == 0:\n",
    "        print('%d iterations' % (i+1))\n",
    "        print('recon_error: %.3f' % np.mean(train_res_recon_error[-100:]))\n",
    "        print('perplexity: %.3f' % np.mean(train_res_perplexity[-100:]))\n",
    "        print('beta',beta)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.ndimage import uniform_filter1d\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Smooth the values\n",
    "train_res_recon_error_smooth = uniform_filter1d(train_res_recon_error, size=10, mode='nearest')\n",
    "train_res_perplexity_smooth = uniform_filter1d(train_res_perplexity, size=10, mode='nearest')\n",
    "betas_smooth = uniform_filter1d(betas, size=10, mode='nearest')\n",
    "entropy_smooth = uniform_filter1d(ent, size=10, mode='nearest')\n",
    "\n",
    "# Create a figure with 3 subplots (1 row, 3 columns)\n",
    "f, ax = plt.subplots(1, 3, figsize=(16, 4))\n",
    "\n",
    "# Plot on the left y-axis (first subplot)\n",
    "ax[0].plot(train_res_recon_error_smooth, label='Smoothed NMSE')\n",
    "ax[0].set_yscale('log')\n",
    "ax[0].set_title('Smoothed NMSE')\n",
    "\n",
    "# Create a twin axis for the first subplot for the beta values\n",
    "ax0_twin = ax[0].twinx()\n",
    "ax0_twin.plot(betas_smooth, color='red', label='Beta')\n",
    "ax0_twin.set_ylabel('Beta')\n",
    "\n",
    "# Plot the perplexity on the second subplot\n",
    "ax[1].plot(train_res_perplexity_smooth, label='Smoothed Average codebook usage (perplexity)')\n",
    "ax[1].set_title('Smoothed Average codebook usage (perplexity)')\n",
    "\n",
    "# Create a twin axis for the second subplot for the beta values\n",
    "ax1_twin = ax[1].twinx()\n",
    "ax1_twin.plot(betas_smooth, color='red', label='Beta')\n",
    "ax1_twin.set_ylabel('Beta')\n",
    "\n",
    "# Plot the entropy on the third subplot\n",
    "ax[2].plot(entropy_smooth, label='Smoothed Entropy')\n",
    "ax[2].set_title('Smoothed Entropy')\n",
    "\n",
    "ax1_twin = ax[2].twinx()\n",
    "ax1_twin.plot(betas_smooth, color='red', label='Beta')\n",
    "ax1_twin.set_ylabel('Beta')\n",
    "# Adjust layout and show the plot\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Codebook Probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = encodings.detach().cpu()\n",
    "avg_probs = torch.mean(enc, dim=0).numpy()\n",
    "\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.bar(range(len(avg_probs)), sorted(avg_probs,reverse=True))\n",
    "plt.xlabel('Index')\n",
    "plt.ylabel('Probability')\n",
    "plt.title('codebook probability')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Codebook activation distribution "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = encodings.reshape(batch_size*7*7,num_embeddings)[:,:].detach()\n",
    "\n",
    "sorted_data, _ = torch.sort(data, dim=1, descending=True)\n",
    "\n",
    "mean_sorted_values = torch.mean(sorted_data, dim=0)\n",
    "\n",
    "data1 = mean_sorted_values.cpu().numpy()\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.bar(range(len(data1)), sorted(data1,reverse=True))\n",
    "plt.xlabel('Index (sorted)')\n",
    "plt.ylabel('Probability')\n",
    "plt.title('codebook activation')\n",
    "plt.show()\n",
    "\n",
    "avg_probs = torch.mean(data, dim=0)\n",
    "print(avg_probs)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
